{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbcf384-165b-4c71-a4e7-b4bd38be3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226f7c95-fd66-4a18-ac77-b53a2dcdfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.opinion.com.bo/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91d9f43-170a-419c-865b-19df29a8fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.opinion.com.bo/video/', 'https://www.opinion.com.bo/blog/section/cobocitos', 'https://www.opinion.com.bo/blog/section/ciencia', 'https://www.opinion.com.bo/blog/section/escena-del-crimen', 'https://www.opinion.com.bo/tag/club-motor', 'https://www.opinion.com.bo/blog/section/revista-asi', 'https://www.opinion.com.bo/tag/tramitologia', 'https://www.opinion.com.bo/blog/section/avisos-necrologicos', 'https://www.opinion.com.bo/blog/section/policial', 'https://www.opinion.com.bo/blog/section/cochabamba', 'https://www.opinion.com.bo/album/', 'https://www.opinion.com.bo/blog/section/deportes', 'https://www.opinion.com.bo/blog/section/cultura', 'https://www.opinion.com.bo/blog/section/tecnologia', 'https://www.opinion.com.bo/blog/section/ramona', 'https://www.opinion.com.bo/blog/section/salud', 'https://www.opinion.com.bo/blog/section/catar', 'https://www.opinion.com.bo/blog/section/virales', 'https://www.opinion.com.bo/blog/section/buenanoche', 'https://www.opinion.com.bo/opinion/', 'https://www.opinion.com.bo/blog/section/informe-especial', 'https://www.opinion.com.bo/blog/section/pais', 'https://www.opinion.com.bo/blog/section/tendencias', 'https://www.opinion.com.bo/blog/section/vida-de-hoy2', 'https://www.opinion.com.bo/blog/section/medio-ambiente', 'https://www.opinion.com.bo/blog/section/especiales', 'https://www.opinion.com.bo/blog/section/mundo', 'https://www.opinion.com.bo/blog/section/colibrito']\n"
     ]
    }
   ],
   "source": [
    "opinion = requests.get(url)\n",
    "if opinion.status_code == 200:\n",
    "    s = BeautifulSoup(opinion.text,'lxml')\n",
    "    secciones = s.find('div', attrs={'class':'main-menu-border'}).find_all('li')\n",
    "    links_secciones = list(set([url + seccion.a.get('href')[1:] for seccion in secciones]))\n",
    "    print(links_secciones)\n",
    "else:\n",
    "    print('Error: problemas con la pagina')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d8a85-ee88-4e54-98fb-753ccc408335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_articulos(soup):\n",
    "    \"\"\"\n",
    "    Funcion que recibe un objeto Beautifulsoup de una seccion de una pagina \n",
    "    y devuelve una lista de URLs de cada nota en la seccion.\n",
    "    \"\"\"\n",
    "    list_notes_links = []\n",
    "    # se obtiene el listado de articulos principales\n",
    "    try:\n",
    "        list_articles = soup.find('div', attrs={'class':'archive-contents'}).find_all('article')\n",
    "        for article in list_articles:\n",
    "            if article.a:\n",
    "                if 'https://www.opinion.com.bo' in article.a.get('href'):\n",
    "                    list_notes_links.append(article.a.get('href'))\n",
    "                else:\n",
    "                    list_notes_links.append('https://www.opinion.com.bo' + article.a.get('href'))      \n",
    "    except:\n",
    "        print('No se pudo obtener los link de los articulos en:', soup.find('meta', attrs={'property':'og:url'}).get('content'))\n",
    "        \n",
    "    return list_notes_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711cd30-7d06-4f00-b814-769db10c39e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_info(s_nota, url):\n",
    "    # se crea un dict vacio para llenarlo de información\n",
    "    info_dict = {}\n",
    "    # Extraemos la sección\n",
    "    seccion = s_nota.find('body').get('data-category')\n",
    "    if seccion:\n",
    "        info_dict['seccion'] = seccion\n",
    "    else:\n",
    "        info_dict['seccion'] = None\n",
    "\n",
    "    # Extraemos el titulo\n",
    "    titulo = s_nota.find('h2', attrs={'class':'title'})\n",
    "    if titulo:\n",
    "        try:\n",
    "            info_dict['titulo'] = titulo.text\n",
    "        except Exception as e:\n",
    "            info_dict['titulo'] = None\n",
    "            print('Error extrayendo titulo: ', e)\n",
    "    else:\n",
    "        info_dict['titulo'] = None\n",
    "        \n",
    "    # Extraemos la fecha\n",
    "    cont_fecha = s_nota.find('script', attrs={'type':'application/ld+json'})\n",
    "    if cont_fecha:\n",
    "        try:\n",
    "            if cont_fecha.text.find('datePublished'):\n",
    "                pos_ini = cont_fecha.text.find('datePublished') + len('datePublished') + 4\n",
    "                pos_fin = pos_ini + len('2022-07-30T18:12:31-04:00')\n",
    "                info_dict['fecha'] = cont_fecha.text[pos_ini:pos_fin]\n",
    "            else:\n",
    "                raise AttributeError('No se pudo encontrar la fecha')\n",
    "        except AttributeError as ae:\n",
    "            info_dict['fecha'] = None\n",
    "            print('Error extrayendo fecha:')\n",
    "            print(ae)\n",
    "    else:\n",
    "        info_dict['fecha'] = None\n",
    "\n",
    "    # Extraemos el resumen\n",
    "    resumen = s_nota.find('div', attrs={'class':'summary'})\n",
    "    if resumen:\n",
    "        try:\n",
    "            info_dict['resumen'] = resumen.text[:-3]\n",
    "        except Exception as e:\n",
    "            info_dict['resumen'] = None\n",
    "            print('Error extrayendo resumen: ',e)\n",
    "    else:\n",
    "        info_dict['resumen'] = None\n",
    "\n",
    "    # Extraemos el autor de la nota\n",
    "    autor = s_nota.find('span', attrs={'class':'author-name'})\n",
    "    if autor: \n",
    "            try:\n",
    "                info_dict['autor'] = autor.a.text[1:-1]\n",
    "            except:\n",
    "                try:\n",
    "                    info_dict['autor'] = autor.text[1:-1]\n",
    "                except Exception as e:\n",
    "                    info_dict['autor'] = None\n",
    "                    print('Error 1er y 2do comando: autor --> dato no extraido')\n",
    "                    print(e)\n",
    "    else:\n",
    "        info_dict['autor'] = None\n",
    "        \n",
    "    # Extraemos el contenido\n",
    "    contenido = s_nota.find('div', attrs={'class':'body'})\n",
    "    try:\n",
    "        if len(contenido.find_all('p')) > 1:\n",
    "            contenido = contenido.find_all('p')\n",
    "            contenido_unido = ''\n",
    "            for parrafo in contenido:\n",
    "                contenido_unido = contenido_unido + parrafo.text + ' '\n",
    "            info_dict['texto'] = contenido_unido.replace('\\xa0', ' ')\n",
    "            \n",
    "        elif len(contenido.find_all('p')) == 1:\n",
    "            contenido = contenido.text.replace('\\n\\n', ' ').replace('\\n','')\n",
    "            info_dict['texto'] = contenido\n",
    "        else:\n",
    "            print(f'Contenido no encontrado en: {url}')\n",
    "            info_dict['texto'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        info_dict['texto'] = None\n",
    "        print('Error extrayendo contenido ',e)\n",
    "        \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaff997a-6e67-4119-8363-4df028e61750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scrapear_nota(url):\n",
    "    try:\n",
    "        nota = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print('Error scrapeando la url: ', url)\n",
    "        print(e)\n",
    "        return None\n",
    "    if nota.status_code != 200:\n",
    "        print(f'Error obteniendo la nota: {url}')\n",
    "        print(f'Status Code: {nota.status_code}')\n",
    "        return None\n",
    "    s_nota = BeautifulSoup(nota.text, 'lxml')\n",
    "    \n",
    "    info_dict = obtener_info(s_nota, url)\n",
    "    info_dict['url'] = url\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4eb2b2-035e-445f-aacf-98d01212566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "notas = []\n",
    "for link in links_secciones:\n",
    "    try:\n",
    "        r = requests.get(link)\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.text, 'lxml')\n",
    "            notas.extend(obtener_articulos(soup))\n",
    "        else:\n",
    "            print('No se pudo obtener la sección', link)\n",
    "    except Exception as e:\n",
    "        print('No se pudo obtener la seccion', link)\n",
    "\n",
    "print(f'Recoleccion de notas terminada: {len(notas)} notas obtenidas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a813968-1288-4fc6-b55e-6249f4424851",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "notas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a4c75-dd64-4fbe-9adb-121d7dd905a0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, nota in enumerate(notas):\n",
    "    print(f'Scrapeando nota {i+1}/{len(notas)}')\n",
    "    data.append(scrapear_nota(nota))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a284a-b3f0-4f63-8476-a0518b0442fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584a5b0-b718-41ea-8520-3551198c2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093fc36-09d7-4f3c-ab60-bed36550a5df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb140f1-2cb8-42b0-9549-bdedf91f6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('notas_opinion.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
